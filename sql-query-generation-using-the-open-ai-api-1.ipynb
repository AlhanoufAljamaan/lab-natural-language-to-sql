{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aad5153-7470-4b09-9722-66216f873d5d",
   "metadata": {},
   "source": [
    "# SQL Query Generation Using the OpenAI API (1)\n",
    "\n",
    "This notebook demonstrates how to leverage GPT-3.5-Turbo to intelligently select the appropriate database tables for SQL query generation based on minimal information - just table names and brief content descriptions. We'll evaluate the model's ability to understand database structure and transform natural language requests into accurate SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fbbdf5b-4d7f-4496-8f40-9d15ea46d023",
   "metadata": {
    "id": "7fbbdf5b-4d7f-4496-8f40-9d15ea46d023"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e480bbfb-9a80-4ea6-b792-067e63ae3148",
   "metadata": {
    "id": "e480bbfb-9a80-4ea6-b792-067e63ae3148"
   },
   "outputs": [],
   "source": [
    "# Function to Call the OpenAI Model\n",
    "\n",
    "# temperature=0 ensures deterministic, consistent responses\n",
    "def return_OAI(user_message, temperature=0,model=\"gpt-3.5-turbo\"):\n",
    "    # Initialize the OpenAI client with your API key\n",
    "    # The API key is assumed to be defined in the OPENAI_API_KEY variable\n",
    "    client = OpenAI(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "    )\n",
    "    \n",
    "    # Create a context list with the user message as a system message\n",
    "    # This approach uses only system messages rather than mixing system and user roles\n",
    "    context = []\n",
    "    context.append({'role':'system', \"content\": user_message})\n",
    "    \n",
    "    # Make the API call to generate a completion\n",
    "    response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=context,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "    \n",
    "    # Return just the content of the first (and only) message in the response\n",
    "    return (response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d5d2bdc-d822-4ed8-815e-b3f223730f15",
   "metadata": {
    "id": "6d5d2bdc-d822-4ed8-815e-b3f223730f15",
    "outputId": "61068bf0-41e3-40d9-b453-b76da5b0f086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       table                                         definition\n",
      "0  employees                      Employee information, name...\n",
      "1     salary                       Salary details for each year\n",
      "2    studies  Educational studies, name of the institution, ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table</th>\n",
       "      <th>definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>employees</td>\n",
       "      <td>Employee information, name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>salary</td>\n",
       "      <td>Salary details for each year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>studies</td>\n",
       "      <td>Educational studies, name of the institution, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       table                                         definition\n",
       "0  employees                      Employee information, name...\n",
       "1     salary                       Salary details for each year\n",
       "2    studies  Educational studies, name of the institution, ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Table and definitions sample\n",
    "data = {'table': ['employees', 'salary', 'studies'],\n",
    "        'definition': ['Employee information, name...',\n",
    "                       'Salary details for each year',\n",
    "                       'Educational studies, name of the institution, type of studies, level']}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "# This code creates a DataFrame with three tables in our database:\n",
    "# 1. \"employees\" - Contains basic employee information\n",
    "# 2. \"salary\" - Contains annual salary information \n",
    "# 3. \"studies\" - Contains educational background details\n",
    "# We'll use these table definitions to test if GPT-3.5-Turbo can identify\n",
    "# which tables are needed to answer specific queries\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "009e681c-d95d-4c9c-b044-5bfd76e3ad95",
   "metadata": {
    "id": "009e681c-d95d-4c9c-b044-5bfd76e3ad95"
   },
   "outputs": [],
   "source": [
    "# Creates a formatted string representation of all tables and their descriptions\n",
    "# Each line follows the format \"table_name: table_description\" , df.iterrows() allows for iterating over the rows of the df\n",
    "text_tables = '\\n'.join([f\"{row['table']}: {row['definition']}\" for index, row in df.iterrows()])\n",
    "\n",
    "# This joins all table definitions into a single string, with each table on a new line\n",
    "# This formatted string will be used as part of the prompt for the GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe03ac25-8d02-4cd1-99a1-be4220c6fd2d",
   "metadata": {
    "id": "fe03ac25-8d02-4cd1-99a1-be4220c6fd2d",
    "outputId": "c1f3aab1-5f26-48fe-fcf9-3780120f5aad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees: Employee information, name...\n",
      "salary: Salary details for each year\n",
      "studies: Educational studies, name of the institution, type of studies, level\n"
     ]
    }
   ],
   "source": [
    "print(text_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7e275ae-f20d-4134-b9b6-d8677dfb544c",
   "metadata": {
    "id": "c7e275ae-f20d-4134-b9b6-d8677dfb544c"
   },
   "outputs": [],
   "source": [
    "# Template for the prompt to query the model about which tables are needed\n",
    "prompt_question_tables = \"\"\"\n",
    "Given the following tables and their content definitions,\n",
    "###Tables\n",
    "{tables}\n",
    "Tell me which tables would be necessary to query with SQL to address the user's question below.\n",
    "Return the table names in a json format.\n",
    "###User Questyion:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# This string template has placeholders that will be filled in:\n",
    "# {tables} - Will be replaced with the text_tables string containing all table definitions\n",
    "# {question} - Will be replaced with the user's natural language query\n",
    "# The model is instructed to respond in JSON format for easy parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1cb5957-2df2-4e5e-9e6a-ace955c9817e",
   "metadata": {
    "id": "b1cb5957-2df2-4e5e-9e6a-ace955c9817e"
   },
   "outputs": [],
   "source": [
    "# Create the complete prompt by filling in the template\n",
    "# This combines the table definitions with a specific user question\n",
    "pqt1 = prompt_question_tables.format(tables=text_tables, question=\"Return The name of the best paid employee\")\n",
    "\n",
    "# This formatted prompt will ask the model to identify which tables are needed\n",
    "# to find the best paid employee from our database structure\n",
    "# The model will need to understand that this requires salary information and employee names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10d30f2b-6c23-4fd6-8038-840fba784cce",
   "metadata": {
    "id": "10d30f2b-6c23-4fd6-8038-840fba784cce",
    "outputId": "9924022c-7b2b-4ec8-e2c2-75bc1c745151",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mreturn_OAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpqt1\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m, in \u001b[0;36mreturn_OAI\u001b[1;34m(user_message, temperature, model)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreturn_OAI\u001b[39m(user_message, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Initialize the OpenAI client with your API key\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# The API key is assumed to be defined in the OPENAI_API_KEY variable\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     client \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOPENAI_API_KEY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Create a context list with the user message as a system message\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# This approach uses only system messages rather than mixing system and user roles\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     context \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\_client.py:110\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    108\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "print(return_OAI(pqt1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e07083-be8f-4cd0-95bd-c4b909422c6b",
   "metadata": {
    "id": "57e07083-be8f-4cd0-95bd-c4b909422c6b"
   },
   "outputs": [],
   "source": [
    "# Create another prompt for a different user question\n",
    "# This question asks about correlation between education and salary\n",
    "pqt3 = prompt_question_tables.format(tables=text_tables,\n",
    "                                    question=\"Return the Education Institution with a higher average salary\")\n",
    "\n",
    "# This prompt will ask the model to identify tables needed to determine\n",
    "# which educational institution's graduates have the highest average salary\n",
    "# This likely requires joining all three tables to connect employees to their\n",
    "# educational backgrounds and respective salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eeb79e-caf1-4f48-9897-168d95d2ae37",
   "metadata": {
    "id": "a0eeb79e-caf1-4f48-9897-168d95d2ae37",
    "outputId": "81d77115-9cad-4284-a228-5368bb9aa6fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"tables\": [\"employees\", \"salary\", \"studies\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(return_OAI(pqt3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321bb9a2-4937-4e9a-a31b-7049cb8f5aa3",
   "metadata": {
    "id": "321bb9a2-4937-4e9a-a31b-7049cb8f5aa3"
   },
   "source": [
    "# Conclusions\n",
    "It is clear that GPT-3.5-Turbo is a model entirely capable of deciding which tables should be used in creating an SQL query.\n",
    "\n",
    "In a more complex system, we may need to refine the definition and conduct several tests before finalizing them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1269aaf2-e203-44da-b3cb-80bf538d67a3",
   "metadata": {
    "id": "1269aaf2-e203-44da-b3cb-80bf538d67a3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5ceb2d-1fab-4bba-9fe3-5e347f858e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
